{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "#from bbtnn import bbtnn as bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.datasets.paul15()\n",
    "sc.pp.recipe_zheng17(adata)\n",
    "sc.pp.pca(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = np.random.choice([0,1,2], adata.shape[0], replace = True)\n",
    "adata.obs[\"timepoints\"] = timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BBTNN(adata = adata, timeseries=\"timepoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-a9ba2bc69ba3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobsm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"X_pca\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-d4fcb5a8a171>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, shuffle_mode)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \"\"\"\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-d4fcb5a8a171>\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, Y, shuffle_mode)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m             verbose=self.verbose)\n\u001b[0m\u001b[0;32m    491\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_history_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    671\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    674\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager_dataset_or_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m       \u001b[1;31m# Make sure that y, sample_weights, validation_split are not passed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1433\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m       shuffle=shuffle)\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m   \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mconvert_to_generator_like\u001b[1;34m(data, batch_size, steps_per_epoch, epochs, shuffle)\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbatches\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m     \"\"\"\n\u001b[1;32m--> 396\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings = model.fit(X = adata.obsm[\"X_pca\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries=\"timepoints\"\n",
    "timepoints = adata.obs[timeseries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'once')\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from ivis.nn.losses import triplet_loss, is_categorical, is_multiclass, is_hinge\n",
    "#from ivis.nn.network import base_network\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, AlphaDropout, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2, l1_l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator\n",
    "import io\n",
    "import json\n",
    "import shutil\n",
    "import multiprocessing\n",
    "from scipy.sparse import issparse\n",
    "import bbknn\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from ivis import Ivis\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "'''\n",
    "## setting GPU No. for training\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2' #use GPU with ID=0\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8 # maximun alloc gpu50% of MEM\n",
    "#config.gpu_options.allow_growth = True #allocate dynamically\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "\n",
    "\n",
    "## setting CPU\n",
    "cpu_config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads = 20, inter_op_parallelism_threads = 20, device_count = {'CPU': 20})\n",
    "#with tf.Session(config = cpu_config) as sess\n",
    "sess = tf.compat.v1.Session(config = cpu_config)\n",
    "sc.set_figure_params(dpi_save = 300, vector_friendly = True)\n",
    "'''\n",
    "\n",
    "\n",
    "def triplet_network(base_network, embedding_dims=2, embedding_l1=0.01, embedding_l2=0.01):\n",
    "    def output_shape(shapes):\n",
    "        shape1, shape2, shape3 = shapes\n",
    "        return (3, shape1[0],)\n",
    "\n",
    "    input_a = Input(shape=base_network.input_shape[1:])\n",
    "    input_p = Input(shape=base_network.input_shape[1:])\n",
    "    input_n = Input(shape=base_network.input_shape[1:])\n",
    "\n",
    "    embeddings = Dense(embedding_dims,\n",
    "                       kernel_regularizer=l1_l2(l1=embedding_l1, l2=embedding_l2))(base_network.output)\n",
    "    network = Model(base_network.input, embeddings)\n",
    "\n",
    "    processed_a = network(input_a)\n",
    "    processed_p = network(input_p)\n",
    "    processed_n = network(input_n)\n",
    "\n",
    "    triplet = Lambda(K.stack,\n",
    "                     output_shape=output_shape,\n",
    "                     name='stacked_triplets')([processed_a,\n",
    "                                               processed_p,\n",
    "                                               processed_n],)\n",
    "    model = Model([input_a, input_p, input_n], triplet)\n",
    "\n",
    "    return model, processed_a, processed_p, processed_n\n",
    "\n",
    "\n",
    "## Base network\n",
    "\n",
    "def base_network(model_name, input_shape):\n",
    "    '''Return the defined base_network defined by the model_name string.\n",
    "    '''\n",
    "    if model_name == 'szubert':\n",
    "        return szubert_base_network(input_shape)\n",
    "    elif model_name == 'hinton':\n",
    "        return hinton_base_network(input_shape)\n",
    "    elif model_name == 'maaten':\n",
    "        return maaten_base_network(input_shape)\n",
    "\n",
    "    raise NotImplementedError(\n",
    "        'Base network {} is not implemented'.format(model_name))\n",
    "\n",
    "\n",
    "def get_base_networks():\n",
    "    return ['szubert', 'hinton', 'maaten']\n",
    "\n",
    "\n",
    "def szubert_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(128, activation='selu',\n",
    "              kernel_initializer='lecun_normal')(inputs)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(128, activation='selu',\n",
    "              kernel_initializer='lecun_normal')(x)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(128, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    return Model(inputs, x)\n",
    "\n",
    "\n",
    "def hinton_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(1000, activation='selu',\n",
    "              kernel_initializer='lecun_normal')(inputs)\n",
    "    x = AlphaDropout(0.2)(x)\n",
    "    x = Dense(500, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = AlphaDropout(0.2)(x)\n",
    "    x = Dense(100, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    return Model(inputs, x)\n",
    "\n",
    "\n",
    "def maaten_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(500, activation='selu',\n",
    "              kernel_initializer='lecun_normal')(inputs)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(500, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(2000, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    return Model(inputs, x)\n",
    "\n",
    "\n",
    "def calculate_global_triplets(\n",
    "adata,\n",
    "timeseries,\n",
    "neighbors_within_batch = 5,\n",
    "n_pcs = 10,\n",
    "approx = True,\n",
    "metric = \"euclidean\",\n",
    "use_faiss = True,\n",
    "n_trees = 10):\n",
    "\n",
    "    timepoints = adata.obs[timeseries]\n",
    "\n",
    "    def Intersection(lst1, lst2):\n",
    "        return set(lst1).intersection(lst2)\n",
    "    \n",
    "    unique_timepoints = np.unique(timepoints).tolist()\n",
    "    list_of_timepoints = []\n",
    "    for i in unique_timepoints:\n",
    "        lst1 = list([i -1, i, i + 1])\n",
    "        final_list = list(Intersection(lst1, unique_timepoints))\n",
    "        list_of_timepoints.append(final_list)\n",
    "\n",
    "    global_triplets = []\n",
    "    for i in range(0, len(list_of_timepoints)):\n",
    "        adjacent_timepoints = list_of_timepoints[i]\n",
    "        subm = adata[adata.obs[timeseries].isin(adjacent_timepoints)]\n",
    "        knn_distances, knn_indices = bbknn.get_graph(subm.obsm[\"X_pca\"], batch_list = subm.obs[timeseries], neighbors_within_batch = 5, n_pcs = 10,\n",
    "                                                         approx = True, metric = \"euclidean\", use_faiss = False, n_trees = 10)\n",
    "       \n",
    "        knn_distances = knn_distances[subm.obs[timeseries] == unique_timepoints[i],:]\n",
    "        knn_indices = knn_indices[subm.obs[timeseries] == unique_timepoints[i],:]\n",
    "       \n",
    "        for j in range(0, knn_indices.shape[0]):\n",
    "            anchor = j\n",
    "           \n",
    "            neighbors = knn_indices[j,:]\n",
    "            rest = list(set(range(0, knn_indices.shape[0])).difference(set(neighbors)))\n",
    "           \n",
    "            positive = int(np.random.choice(neighbors, 1))\n",
    "            negative = int(np.random.choice(rest, 1))\n",
    "           \n",
    "            cells = subm.obs_names\n",
    "           \n",
    "            global_triplets.append([cells[anchor], cells[positive], cells[negative]])\n",
    "    return(global_triplets)\n",
    "\n",
    "class KnnTripletGenerator_timeseries(Sequence):\n",
    "    def __init__(self, timeseries, adata, batch_size=32):\n",
    "        self.adata = adata\n",
    "        self.timeseries = timeseries\n",
    "        self.placeholder_labels = np.empty(batch_size, dtype=np.uint8)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = range(idx * self.batch_size, min((idx + 1) * self.batch_size, self.X.shape[0]))\n",
    "\n",
    "        placeholder_labels = self.placeholder_labels[:len(batch_indices)]\n",
    "        triplet_batch = [self.knn_triplet_from_neighbour_list(row_index, adata, global_triplets[row_index]) for row_index in batch_indices]\n",
    "\n",
    "        return tuple([triplet_batch[:, 0], triplet_batch[:, 1], triplet_batch[:, 2]]), placeholder_labels\n",
    "\n",
    "    def knn_triplet_from_neighbour_list(self, adata, row_index, triplet_list):\n",
    "        \"\"\" A random (unweighted) positive example chosen. \"\"\"\n",
    "        triplets = []\n",
    "\n",
    "        anchor, positive, negative = global_triplets[row_index]\n",
    "\n",
    "        anchor = adata.obs_names == anchor\n",
    "        positive = adata.obs_names == positive\n",
    "        negative = adata.obs_names == negative\n",
    "        \n",
    "        triplets += [anchor, positive, negative]\n",
    "\n",
    "        return triplets\n",
    "    \n",
    "\n",
    "class KnnTripletGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, X, neighbour_matrix, batch_size=32):\n",
    "        self.X = X\n",
    "        #self.batch = batch\n",
    "        self.neighbour_matrix = neighbour_matrix\n",
    "        self.batch_size = batch_size\n",
    "        self.placeholder_labels = np.empty(batch_size, dtype=np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.X.shape[0] / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = range(idx * self.batch_size, min((idx + 1) * self.batch_size, self.X.shape[0]))\n",
    "\n",
    "        placeholder_labels = self.placeholder_labels[:len(batch_indices)]\n",
    "        triplet_batch = [self.knn_triplet_from_neighbour_list(row_index, self.neighbour_matrix[row_index]) for row_index in batch_indices]\n",
    "\n",
    "        if (issparse(self.X)):\n",
    "            triplet_batch = [[e.toarray()[0] for e in t] for t in triplet_batch]\n",
    "        triplet_batch = np.array(triplet_batch)\n",
    "\n",
    "        return tuple([triplet_batch[:, 0], triplet_batch[:, 1], triplet_batch[:, 2]]), placeholder_labels\n",
    "    \n",
    "    def knn_triplet_from_neighbour_list(self, row_index, neighbour_list):\n",
    "        \"\"\" A random (unweighted) positive example chosen. \"\"\"\n",
    "\n",
    "        # Take a random neighbour as positive\n",
    "        neighbour_ind = np.random.choice(neighbour_list)\n",
    "\n",
    "        # Take a random non-neighbour as negative\n",
    "        # Pick a random index until one fits constraint. An optimization.\n",
    "        negative_ind = np.random.randint(0, self.X.shape[0])\n",
    "        while negative_ind  in neighbour_list:\n",
    "            negative_ind = np.random.randint(0, self.X.shape[0])\n",
    "\n",
    "        triplets += [self.X[row_index],\n",
    "                     self.X[neighbour_ind],\n",
    "                     self.X[negative_ind]]\n",
    "        return triplets\n",
    "    \n",
    "    \n",
    "class LabeledKnnTripletGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, X, Y, neighbour_matrix, batch_size=32):\n",
    "        self.X, self.Y = X, Y\n",
    "        #self.batch = batch\n",
    "        self.neighbour_matrix = neighbour_matrix\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.X.shape[0] / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = range(idx * self.batch_size, min((idx + 1) * self.batch_size, self.X.shape[0]))\n",
    "\n",
    "        label_batch = self.Y[batch_indices]\n",
    "        triplet_batch = [self.knn_triplet_from_neighbour_list(row_index, self.neighbour_matrix[row_index]) for row_index in batch_indices]\n",
    "\n",
    "        if (issparse(self.X)):\n",
    "            triplet_batch = [[e.toarray()[0] for e in t] for t in triplet_batch]\n",
    "\n",
    "        triplet_batch = np.array(triplet_batch)\n",
    "\n",
    "        return tuple([triplet_batch[:, 0], triplet_batch[:, 1], triplet_batch[:, 2]]), tuple([np.array(label_batch), np.array(label_batch)])\n",
    "\n",
    "    def knn_triplet_from_neighbour_list(self, row_index, neighbour_list):\n",
    "        \"\"\" A random (unweighted) positive example chosen. \"\"\"\n",
    "        triplets = []\n",
    "\n",
    "        # Take a random neighbour as positive\n",
    "        neighbour_ind = np.random.choice(neighbour_list)\n",
    "\n",
    "        # Take a random non-neighbour as negative\n",
    "        # Pick a random index until one fits constraint. An optimization.\n",
    "        negative_ind = np.random.randint(0, self.X.shape[0])\n",
    "        while negative_ind  in neighbour_list:\n",
    "            negative_ind = np.random.randint(0, self.X.shape[0])\n",
    "\n",
    "        triplets += [self.X[row_index],\n",
    "                     self.X[neighbour_ind],\n",
    "                     self.X[negative_ind]]\n",
    "        return triplets\n",
    "\n",
    "\n",
    "def generator_from_index(X, Y, k = 15, adata = adata, batch_size = 128, batch_list = 4, timeseries = None, search_k=-1, verbose=1):\n",
    "        if k >= X.shape[0] - 1:\n",
    "                raise Exception('''k value greater than or equal to (num_rows - 1)(k={}, rows={}). Lower k to a smaller value.'''.format(k, X.shape[0]))\n",
    "\n",
    "        if batch_size > X.shape[0]:\n",
    "                raise Exception('''batch_size value larger than num_rows in dataset (batch_size={}, rows={}). Lower batch_size to a smaller value.'''.format(batch_size, X.shape[0]))\n",
    "\n",
    "        if timeseries is None:\n",
    "            if Y is None:\n",
    "                return KnnTripletGenerator(X = X,  neighbour_matrix = neighbour_matrix, batch_size=batch_size)\n",
    "            else:\n",
    "                return LabeledKnnTripletGenerator(X = X, Y = Y,  neighbour_matrix = neighbour_matrix, batch_size=batch_size)\n",
    "        else:\n",
    "            return KnnTripletGenerator_timeseries(adata = adata, batch_size=batch_size, timeseries = timeseries)\n",
    "\n",
    "class BBTNN(BaseEstimator):\n",
    "    def __init__(self, adata, embedding_dims=2, k=150, distance='pn', batch_size=128, batch_list = 4, timeseries = 4 ,\n",
    "                 epochs=1000, n_epochs_without_progress=50,\n",
    "                 margin=1, ntrees=50, search_k=-1,\n",
    "                 model='szubert',supervision_metric='sparse_categorical_crossentropy',\n",
    "                 supervision_weight=0.8,\n",
    "                 callbacks=[], eager_execution=False, verbose=1):\n",
    "\n",
    "        self.adata = adata\n",
    "        self.timeseries = timeseries\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.k = k\n",
    "        self.distance = distance\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_list = batch_list \n",
    "        self.epochs = epochs\n",
    "        self.n_epochs_without_progress = n_epochs_without_progress\n",
    "        self.margin = margin\n",
    "        self.ntrees = ntrees\n",
    "        self.search_k = search_k\n",
    "        self.model_def = model\n",
    "        self.model_ = None\n",
    "        self.encoder = None\n",
    "        self.supervision_metric = supervision_metric\n",
    "        self.supervision_weight = supervision_weight\n",
    "        self.loss_history_ = []\n",
    "        self.callbacks = callbacks\n",
    "        for callback in self.callbacks:\n",
    "            if isinstance(callback, ModelCheckpoint):\n",
    "                callback = callback.register_ivis_model(self)\n",
    "        self.eager_execution = eager_execution\n",
    "        if not eager_execution:\n",
    "            tf.compat.v1.disable_eager_execution()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\" Return object serializable variable dict \"\"\"\n",
    "\n",
    "        state = dict(self.__dict__)\n",
    "        if 'model_' in state:\n",
    "            state['model_'] = None\n",
    "        if 'encoder' in state:\n",
    "            state['encoder'] = None\n",
    "        if 'supervised_model_' in state:\n",
    "            state['supervised_model_'] = None\n",
    "        if 'callbacks' in state:\n",
    "            state['callbacks'] = []\n",
    "        if not isinstance(state['model_def'], str):\n",
    "            state['model_def'] = None\n",
    "        return state\n",
    "\n",
    "    def _fit(self, X, Y=None, shuffle_mode=True):\n",
    "\n",
    "        if self.timeseries != None:\n",
    "            global_triplets = calculate_global_triplets(adata = self.adata, timeseries = self.timeseries)\n",
    "\n",
    "        datagen = generator_from_index(X, Y,\n",
    "                                       adata = self.adata,\n",
    "                                       k = self.k,\n",
    "                                       timeseries = self.timeseries,\n",
    "                                       batch_size = self.batch_size,\n",
    "                                       batch_list = self.batch_list,\n",
    "                                       search_k=self.search_k,\n",
    "                                       verbose=self.verbose)\n",
    "\n",
    "        loss_monitor = 'loss'\n",
    "        try:\n",
    "            triplet_loss_func = triplet_loss(distance=self.distance,\n",
    "                                             margin=self.margin)\n",
    "        except KeyError:\n",
    "            raise ValueError('Loss function `{}` not implemented.'.format(self.distance))\n",
    "\n",
    "        if self.model_ is None:\n",
    "            if type(self.model_def) is str:\n",
    "                input_size = (X.shape[-1],)\n",
    "                self.model_, anchor_embedding, _, _ = \\\n",
    "                    triplet_network(base_network(self.model_def, input_size),\n",
    "                                    embedding_dims=self.embedding_dims)\n",
    "            else:\n",
    "                self.model_, anchor_embedding, _, _ = \\\n",
    "                    triplet_network(self.model_def,\n",
    "                                    embedding_dims=self.embedding_dims)\n",
    "\n",
    "            if Y is None:\n",
    "                self.model_.compile(optimizer='adam', loss=triplet_loss_func)\n",
    "            else:\n",
    "                if is_categorical(self.supervision_metric):\n",
    "                    if not is_multiclass(self.supervision_metric):\n",
    "                        if not is_hinge(self.supervision_metric):\n",
    "                            # Binary logistic classifier\n",
    "                            if len(Y.shape) > 1:\n",
    "                                self.n_classes = Y.shape[-1]\n",
    "                            else:\n",
    "                                self.n_classes = 1\n",
    "                            supervised_output = Dense(self.n_classes, activation='sigmoid',\n",
    "                                                      name='supervised')(anchor_embedding)\n",
    "                        else:\n",
    "                            # Binary Linear SVM output\n",
    "                            if len(Y.shape) > 1:\n",
    "                                self.n_classes = Y.shape[-1]\n",
    "                            else:\n",
    "                                self.n_classes = 1\n",
    "                            supervised_output = Dense(self.n_classes, activation='linear',\n",
    "                                                      name='supervised',\n",
    "                                                      kernel_regularizer=regularizers.l1(l1=0.01))(anchor_embedding)\n",
    "                    else:\n",
    "                        if not is_hinge(self.supervision_metric):\n",
    "                            validate_sparse_labels(Y)\n",
    "                            self.n_classes = len(np.unique(Y[Y != np.array(-1)]))\n",
    "                            # Softmax classifier\n",
    "                            supervised_output = Dense(self.n_classes, activation='softmax',\n",
    "                                                      name='supervised')(anchor_embedding)\n",
    "                        else:\n",
    "                            self.n_classes = len(np.unique(Y, axis=0))\n",
    "                            # Multiclass Linear SVM output\n",
    "                            supervised_output = Dense(self.n_classes, activation='linear',\n",
    "                                                      name='supervised',\n",
    "                                                      kernel_regularizer=regularizers.l1(l1=0.01))(anchor_embedding)\n",
    "                else:\n",
    "                    # Regression\n",
    "                    if len(Y.shape) > 1:\n",
    "                        self.n_classes = Y.shape[-1]\n",
    "                    else:\n",
    "                        self.n_classes = 1\n",
    "                    supervised_output = Dense(self.n_classes, activation='linear',\n",
    "                                              name='supervised')(anchor_embedding)\n",
    "\n",
    "                supervised_loss = keras.losses.get(self.supervision_metric)\n",
    "                if self.supervision_metric == 'sparse_categorical_crossentropy':\n",
    "                    supervised_loss = semi_supervised_loss(supervised_loss)\n",
    "\n",
    "                final_network = Model(inputs=self.model_.inputs,\n",
    "                                      outputs=[self.model_.output,\n",
    "                                               supervised_output])\n",
    "                self.model_ = final_network\n",
    "                self.model_.compile(\n",
    "                    optimizer='adam',\n",
    "                    loss={\n",
    "                        'stacked_triplets': triplet_loss_func,\n",
    "                        'supervised': supervised_loss\n",
    "                         },\n",
    "                    loss_weights={\n",
    "                        'stacked_triplets': 1 - self.supervision_weight,\n",
    "                        'supervised': self.supervision_weight})\n",
    "\n",
    "                # Store dedicated classification model\n",
    "                supervised_model_input = Input(shape=(X.shape[-1],))\n",
    "                embedding = self.model_.layers[3](supervised_model_input)\n",
    "                softmax_out = self.model_.layers[-1](embedding)\n",
    "\n",
    "                self.supervised_model_ = Model(supervised_model_input, softmax_out)\n",
    "\n",
    "        self.encoder = self.model_.layers[3]\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print('Training neural network')\n",
    "\n",
    "        hist = self.model_.fit(\n",
    "            datagen,\n",
    "            epochs=self.epochs,\n",
    "            callbacks=[callback for callback in self.callbacks] +\n",
    "                      [EarlyStopping(monitor=loss_monitor,\n",
    "                       patience=self.n_epochs_without_progress)],\n",
    "            shuffle=shuffle_mode,\n",
    "            workers = 10,\n",
    "            verbose=self.verbose)\n",
    "        self.loss_history_ += hist.history['loss']\n",
    "\n",
    "    \n",
    "    def fit(self, X, Y=None, shuffle_mode=True):\n",
    "        \"\"\"Fit an ivis model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Data to be embedded.\n",
    "        Y : array, shape (n_samples)\n",
    "            Optional array for supervised dimentionality reduction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        returns an instance of self\n",
    "        \"\"\"\n",
    "\n",
    "        self._fit(X, Y, shuffle_mode)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, Y=None, shuffle_mode=True):\n",
    "        \"\"\"Fit to data then transform\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Data to be embedded.\n",
    "        Y : array, shape (n_samples)\n",
    "            Optional array for supervised dimentionality reduction.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : transformed array, shape (n_samples, embedding_dims)\n",
    "            Embedding of the new data in low-dimensional space.\n",
    "        \"\"\"\n",
    "\n",
    "        self.fit(X, Y, shuffle_mode)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X into the existing embedded space and return that\n",
    "        transformed output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            New data to be transformed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, embedding_dims)\n",
    "            Embedding of the new data in low-dimensional space.\n",
    "        \"\"\"\n",
    "\n",
    "        embedding = self.encoder.predict(X, verbose=self.verbose)\n",
    "        return embedding\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        \"\"\"Passes X through classification network to obtain predicted\n",
    "        supervised values. Only applicable when trained in\n",
    "        supervised mode.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Data to be passed through classification network.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, embedding_dims)\n",
    "            Softmax class probabilities of the data.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.supervised_model_ is None:\n",
    "            raise Exception(\"Model was not trained in classification mode.\")\n",
    "\n",
    "        softmax_output = self.supervised_model_.predict(X, verbose=self.verbose)\n",
    "        return softmax_output\n",
    "    \n",
    "    \n",
    "def semi_supervised_loss(loss_function):\n",
    "    def new_loss_function(y_true, y_pred):\n",
    "        mask = tf.cast(~tf.math.equal(y_true, -1), tf.float32)\n",
    "        y_true_pos = tf.nn.relu(y_true)\n",
    "        loss = loss_function(y_true_pos, y_pred)\n",
    "        masked_loss = loss * mask\n",
    "        return masked_loss\n",
    "    new_func = new_loss_function\n",
    "    new_func.__name__ = loss_function.__name__\n",
    "    return new_func\n",
    "\n",
    "def validate_sparse_labels(Y):\n",
    "    if not zero_indexed(Y):\n",
    "        raise ValueError('Ensure that your labels are zero-indexed')\n",
    "    if not consecutive_indexed(Y):\n",
    "        raise ValueError('Ensure that your labels are indexed consecutively')\n",
    "        \n",
    "def zero_indexed(Y):\n",
    "    if min(abs(Y)) != 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def consecutive_indexed(Y):\n",
    "    \"\"\" Assumes that Y is zero-indexed. \"\"\"\n",
    "    n_classes = len(np.unique(Y[Y != np.array(-1)]))\n",
    "    if max(Y) >= n_classes:\n",
    "        return False\n",
    "    return True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
